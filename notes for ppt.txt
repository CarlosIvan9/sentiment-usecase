1- We looked for models hosted elsewhere to not have to possess large memory to store the model, as well as gpus. For 
hg that is why we opted for the inference api instead of downloading the models

You can access the sentiment classifier via both a post request or an html

curl to use in cmd:
curl -X POST https://sentiment-usecase.onrender.com/predict -H "Content-Type: application/json" -d '{ "review": "my wife is amazing" }'

script to log models can be used for any model (script to generate predictions outputs them in a unified format). hence we run it with argparse

recycle code for different runs

zero shot is adapted exclusively for movie reviews. sentiment methods to overall sentiment of the input

Some models like meta-llama/Meta-Llama-3-70B-Instruct where not tried due to very high latency (~49 seconds per input)

decoders are too slow bc i had to add sleep due to rate limits on my free account

feed llm with more than 1 review at a time could have worked for rate limits