{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='-'):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33f6725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1(df):\n",
    "    # Convert string labels to numeric: 1 if 'positive', 0 if 'negative'\n",
    "    y_true = (df[\"Target\"] == \"positive\").astype(int)\n",
    "    y_pred = df[\"positive_score\"]\n",
    "\n",
    "    # Calculate L1 loss\n",
    "    l1_loss = np.abs(y_true - y_pred).mean()\n",
    "    return l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_per_label(df):\n",
    "    # Convert string labels to numeric\n",
    "    y_pred = df[\"positive_score\"]\n",
    "\n",
    "    # Create a mask for each class\n",
    "    positive_mask = df[\"Target\"] == \"positive\"\n",
    "    negative_mask = df[\"Target\"] == \"negative\"\n",
    "\n",
    "    # MAE for positive samples (where true label = 1)\n",
    "    mae_positive = np.abs(y_pred[positive_mask] - 1).mean()\n",
    "\n",
    "    # MAE for negative samples (where true label = 0)\n",
    "    mae_negative = np.abs(y_pred[negative_mask] - 0).mean()\n",
    "\n",
    "    return mae_positive, mae_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_to_mlflow(predictions, metadata):\n",
    "\n",
    "    class_metrics=classification_report(predictions.Target, predictions.Prediction, output_dict=True)\n",
    "    flat_class_metrics =  flatten_dict(class_metrics)\n",
    "\n",
    "    # Create scores distribution plot\n",
    "    sns.histplot(data=predictions, x=\"positive_score\", hue=\"Target\", common_norm=False, bins=15)\n",
    "    plt.title(\"KDE by Category\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    # Save plot to in-memory bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', dpi=300)\n",
    "    buf.seek(0)  # rewind buffer to the beginning\n",
    "    plt.close()  # close the plot to free memory\n",
    "    # Convert BytesIO to PIL Image\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    with mlflow.start_run(run_name=metadata['model']+'/'+metadata['adaptations']):\n",
    "        # Log metrics\n",
    "        for akey in flat_class_metrics.keys():\n",
    "            mlflow.log_metric(akey, flat_class_metrics[akey])\n",
    "\n",
    "        mlflow.log_metric(\"inference_time\", metadata['inference_time'])\n",
    "        mlflow.log_metric(\"MAE\", get_l1(predictions))\n",
    "\n",
    "        mae_positive, mae_negative  = l1_per_label(predictions)\n",
    "        mlflow.log_metric(\"MAE_positive\", mae_positive)\n",
    "        mlflow.log_metric(\"MAE_negative\", mae_negative)\n",
    "\n",
    "        # Log plots\n",
    "        mlflow.log_image(img, \"scores_distribution.png\")\n",
    "\n",
    "        # Log metadata\n",
    "        mlflow.set_tag(\"model\", metadata['model'])\n",
    "        mlflow.set_tag(\"adaptations\", metadata['adaptations'])\n",
    "        mlflow.set_tag(\"other_comments\", metadata['other_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71594296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions_and_metadata(model_name):\n",
    "\n",
    "    # Make output dir\n",
    "    try:\n",
    "        # Works in regular Python scripts\n",
    "        base_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        # Fallback for Jupyter notebooks and interactive shells\n",
    "        base_dir = Path().resolve()\n",
    "\n",
    "    path_predictions = base_dir / \"data\" / 'outputs' / 'runs' / model_name\n",
    "\n",
    "    predictions = pd.read_csv(path_predictions / 'predictions.csv' , sep=';')\n",
    "    with open(path_predictions / \"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    return predictions, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c07b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'twitter-roberta'\n",
    "experiment_name = \"sentiment-usecase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, metadata = load_predictions_and_metadata(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/22 10:28:30 INFO mlflow.tracking.fluent: Experiment with name 'sentiment-usecase' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/Carlos%20Ivan/Documents/Projects/sentiment-usecase/mlruns/558195525818841765', creation_time=1750580910082, experiment_id='558195525818841765', last_update_time=1750580910082, lifecycle_stage='active', name='sentiment-usecase', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95839098",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_to_mlflow(predictions, metadata)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661bd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb06f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5cacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
