{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a93376f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "my_token='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'\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    try:\n",
    "        # Works in regular Python scripts\n",
    "        base_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        # Fallback for Jupyter notebooks and interactive shells\n",
    "        base_dir = Path().resolve()\n",
    "\n",
    "    # Now use it to build your file path\n",
    "    data_path = base_dir / \"data\" / 'inputs' / \"IMDB-movie-reviews.csv\"\n",
    "\n",
    "    # Read file\n",
    "    data = pd.read_csv(data_path, sep=';', encoding='latin-1')\n",
    "\n",
    "\n",
    "    data.rename(columns={'sentiment':'Target'}, inplace=True)\n",
    "\n",
    "    data['review_index'] = data.index\n",
    "    return data \n",
    "\n",
    "\n",
    "def get_sentiment(reviews):\n",
    "\n",
    "    # Make sure input is a list (output of hf is 3 classes if a string is given, or just the top class if a list is given)\n",
    "    if not isinstance(reviews, list):\n",
    "        reviews = [reviews]\n",
    "    client=InferenceClient( #3 seconds per input\n",
    "        model='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        token=my_token\n",
    "        )\n",
    "    \n",
    "    prompt=\"\"\"Predict whether the following document is a positive or negative movie review:\n",
    "    [REVIEW]\n",
    "\n",
    "    If it is positive, return 1, and if it is negative return 0. Do not give any other answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inference for all reviews\n",
    "    all_rows = []\n",
    "    for i, review in enumerate(reviews):\n",
    "        review = review[:4010] # truncation of long reviews to the maximum we have seen\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in movie reviews\"},\n",
    "            {\"role\": \"user\", \"content\": prompt.replace(\"[REVIEW]\", review)},\n",
    "        ]\n",
    "\n",
    "        output = client.chat_completion(messages, max_tokens=20)\n",
    "        print(i)\n",
    "        time.sleep(5)\n",
    "        output = output.choices[0].message.content\n",
    "        all_rows.append({\n",
    "            \"review_index\": i,\n",
    "            \"review\" : review, \n",
    "            \"positive_score\": int(output)\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    outputs_list = ['positive' if score > 0.5 else 'negative' for score in df['positive_score']]\n",
    "    df['Prediction']=outputs_list\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_outputs(data, predictions, model_name, adaptations, inference_time, other_comments):\n",
    "\n",
    "    # Make output dir\n",
    "    try:\n",
    "        # Works in regular Python scripts\n",
    "        base_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        # Fallback for Jupyter notebooks and interactive shells\n",
    "        base_dir = Path().resolve()\n",
    "    # Now use it to build file path to store predictions and metadata\n",
    "    path_outputs = base_dir / \"data\" / 'outputs' / 'runs' / model_name\n",
    "    # Create the directory (and parents if they don't exist)\n",
    "    path_outputs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Add target to the predictions df\n",
    "    output_df=data.merge(predictions.drop(columns=['review']), how='left' , on='review_index')\n",
    "    output_df.drop(columns=['review_index'], inplace=True)\n",
    "\n",
    "    # Save predictions\n",
    "    output_df.to_csv(path_outputs / 'predictions.csv', index=False, sep=';')\n",
    "\n",
    "    # Create metadata file and save it\n",
    "    metadata = { \n",
    "        'model':model_name,\n",
    "        'adaptations': adaptations,\n",
    "        'inference_time': inference_time,\n",
    "        'other_comments': other_comments\n",
    "    }\n",
    "    # Save to JSON file\n",
    "    with open(path_outputs / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e25007b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'generative-llama'\n",
    "adaptations = ''\n",
    "other_comments = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "363935c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "predictions = get_sentiment(list(data.review)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "353e7bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_index</th>\n",
       "      <th>review</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_index                                             review  \\\n",
       "0             0  One of the other reviewers has mentioned that ...   \n",
       "1             1  A wonderful little production. <br /><br />The...   \n",
       "2             2  I thought this was a wonderful way to spend ti...   \n",
       "\n",
       "   positive_score Prediction  \n",
       "0               1   positive  \n",
       "1               1   positive  \n",
       "2               1   positive  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e518236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions (Request ID: Root=1-6859a521-1138b3532cde58a43deb5e9a;b757eb28-5e9f-4c87-843b-42f81cd33dcf)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Carlos Ivan\\Documents\\Projects\\sentiment-usecase\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Carlos Ivan\\Documents\\Projects\\sentiment-usecase\\.venv\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m load_test_data()\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[1;32mIn[108], line 59\u001b[0m, in \u001b[0;36mget_sentiment\u001b[1;34m(reviews)\u001b[0m\n\u001b[0;32m     52\u001b[0m review \u001b[38;5;241m=\u001b[39m review[:\u001b[38;5;241m4010\u001b[39m] \u001b[38;5;66;03m# truncation of long reviews to the maximum we have seen\u001b[39;00m\n\u001b[0;32m     54\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an expert in movie reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     56\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[REVIEW]\u001b[39m\u001b[38;5;124m\"\u001b[39m, review)},\n\u001b[0;32m     57\u001b[0m ]\n\u001b[1;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m     61\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Carlos Ivan\\Documents\\Projects\\sentiment-usecase\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:924\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[0;32m    896\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    916\u001b[0m }\n\u001b[0;32m    917\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    918\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    919\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    922\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    923\u001b[0m )\n\u001b[1;32m--> 924\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carlos Ivan\\Documents\\Projects\\sentiment-usecase\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:280\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 280\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Carlos Ivan\\Documents\\Projects\\sentiment-usecase\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions (Request ID: Root=1-6859a521-1138b3532cde58a43deb5e9a;b757eb28-5e9f-4c87-843b-42f81cd33dcf)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "data = load_test_data()\n",
    "\n",
    "start = time.time()\n",
    "predictions = get_sentiment(list(data.review))\n",
    "end = time.time()\n",
    "inference_time = end - start\n",
    "\n",
    "save_outputs(data, predictions, model_name, adaptations, inference_time, other_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_test_data()\n",
    "\n",
    "    start = time.time()\n",
    "    predictions = get_sentiment(list(data.review))\n",
    "    end = time.time()\n",
    "    inference_time = end - start\n",
    "\n",
    "    save_outputs(data, predictions, model_name, adaptations, inference_time, other_comments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
